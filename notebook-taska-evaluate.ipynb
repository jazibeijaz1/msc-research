{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluating fine-tuned model for Subtask A","metadata":{"id":"rEJBSTyZIrIb"}},{"cell_type":"markdown","source":"Set the model to be used. For evaluating subtask A we use our fine-tuned model. ","metadata":{"id":"kTCFado4IrIc"}},{"cell_type":"code","source":"model_checkpoint = \"JazibEijaz/bert-base-uncased-finetuned-semeval2020-task4a\"","metadata":{"id":"zVvslsfMIrIh","execution":{"iopub.status.busy":"2021-12-06T08:07:25.746598Z","iopub.execute_input":"2021-12-06T08:07:25.746940Z","iopub.status.idle":"2021-12-06T08:07:25.771083Z","shell.execute_reply.started":"2021-12-06T08:07:25.746851Z","shell.execute_reply":"2021-12-06T08:07:25.770367Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{"id":"whPRbBNbIrIl"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndatasets = load_dataset('csv', data_files={'validation': '../input/semeval-test/subtaskA.csv'})","metadata":{"id":"s_AY1ATSIrIq","outputId":"fd0578d1-8895-443d-b56f-5908de9f1b6b","execution":{"iopub.status.busy":"2021-12-06T08:07:25.772895Z","iopub.execute_input":"2021-12-06T08:07:25.773182Z","iopub.status.idle":"2021-12-06T08:07:28.275233Z","shell.execute_reply.started":"2021-12-06T08:07:25.773143Z","shell.execute_reply":"2021-12-06T08:07:28.274568Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the data","metadata":{"id":"n9qywopnIrJH"}},{"cell_type":"markdown","source":"Before we can feed those texts to our model, we need to tokenize it. This is done by a Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n\nTo do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n\n- we get a tokenizer that corresponds to the model architecture we want to use,\n- we download the vocabulary used when pretraining this specific checkpoint.\n","metadata":{"id":"YVx71GdAIrJH"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","metadata":{"id":"eXNLu_-nIrJI","execution":{"iopub.status.busy":"2021-12-06T08:07:28.276516Z","iopub.execute_input":"2021-12-06T08:07:28.277189Z","iopub.status.idle":"2021-12-06T08:07:37.852001Z","shell.execute_reply.started":"2021-12-06T08:07:28.277149Z","shell.execute_reply":"2021-12-06T08:07:37.850985Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"We can then write the function that will preprocess our samples. The tricky part is to put all the possible pairs of sentences in two big lists before passing them to the tokenizer, then un-flatten the result so that each example has three input ids, attentions masks, etc.\n\nWhen calling the `tokenizer`, we use the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model.","metadata":{"id":"2C0hcmp9IrJQ"}},{"cell_type":"code","source":"def preprocess_function(examples):\n    # uncomment the following line to evaluate using setup 1\n    sentences = [[examples[\"sent0\"][i], examples[\"sent1\"][i]] for i in range(len(examples[\"sent0\"]))]\n    \n    # uncomment the following line to evaluate using setup 2\n    # sentences = [[f\"{examples['sent0'][i]} [SEP] {examples['sent1'][i]}\", f\"{examples['sent1'][i]} [SEP] {examples['sent0'][i]}\"] for i in range(len(examples[\"sent0\"]))]\n    \n    # Flatten everything\n    sentences = sum(sentences, [])\n    \n    # Tokenize\n    \n    # uncomment the following line to evaluate using setup 1\n    tokenized_examples = tokenizer(sentences, truncation=True)\n    \n    # uncomment the following line to evaluate using setup 2\n    # tokenized_examples = tokenizer(sentences, truncation=True, add_special_tokens=True)\n    \n    # Un-flatten\n    return {k: [v[i:i+2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}","metadata":{"id":"vc0BSBLIIrJQ","execution":{"iopub.status.busy":"2021-12-06T08:07:37.854335Z","iopub.execute_input":"2021-12-06T08:07:37.854606Z","iopub.status.idle":"2021-12-06T08:07:37.864457Z","shell.execute_reply.started":"2021-12-06T08:07:37.854561Z","shell.execute_reply":"2021-12-06T08:07:37.862666Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We can apply this function on all the examples in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of the `dataset`.","metadata":{"id":"zS-6iXTkIrJT"}},{"cell_type":"code","source":"encoded_datasets = datasets.map(preprocess_function, batched=True)","metadata":{"id":"DDtsaJeVIrJT","outputId":"aa4734bf-4ef5-4437-9948-2c16363da719","execution":{"iopub.status.busy":"2021-12-06T08:07:37.865847Z","iopub.execute_input":"2021-12-06T08:07:37.866122Z","iopub.status.idle":"2021-12-06T08:07:38.351749Z","shell.execute_reply.started":"2021-12-06T08:07:37.866080Z","shell.execute_reply":"2021-12-06T08:07:38.350990Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the model","metadata":{"id":"545PP3o8IrJV"}},{"cell_type":"markdown","source":"Now that our data is ready, we can download the pretrained model and evaluate it. Since our task is about mutliple choice, we use the `AutoModelForMultipleChoice` class. Like with the tokenizer, the `from_pretrained` method will download the model for us.","metadata":{"id":"FBiW8UpKIrJW"}},{"cell_type":"code","source":"from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\nmodel = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)","metadata":{"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132","execution":{"iopub.status.busy":"2021-12-06T08:07:38.353302Z","iopub.execute_input":"2021-12-06T08:07:38.353789Z","iopub.status.idle":"2021-12-06T08:07:54.714008Z","shell.execute_reply.started":"2021-12-06T08:07:38.353751Z","shell.execute_reply":"2021-12-06T08:07:54.713252Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the outputs of the model, and all other arguments are optional:","metadata":{"id":"_N8urzhyIrJY"}},{"cell_type":"code","source":"batch_size = 128\nargs = TrainingArguments(\n    f\"taskA\",\n    per_device_eval_batch_size=batch_size,\n)","metadata":{"id":"Bliy8zgjIrJY","execution":{"iopub.status.busy":"2021-12-06T08:07:54.715440Z","iopub.execute_input":"2021-12-06T08:07:54.715684Z","iopub.status.idle":"2021-12-06T08:07:54.767287Z","shell.execute_reply.started":"2021-12-06T08:07:54.715650Z","shell.execute_reply":"2021-12-06T08:07:54.766608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Then we need to tell our `Trainer` how to form batches from the pre-processed inputs. We haven't done any padding yet because we will pad each batch to the maximum length inside the batch (instead of doing so with the maximum length of the whole dataset). This will be the job of the *data collator*. A data collator takes a list of examples and converts them to a batch (by, in our case, applying padding). Since there is no data collator in the library that works on our specific problem, we will write one, adapted from the `DataCollatorWithPadding`:","metadata":{"id":"km3pGVdTIrJc"}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    \"\"\"\n    Data collator that will dynamically pad the inputs for multiple choice received.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0][\"input_ids\"])\n        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        \n        # Un-flatten\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        # Add back labels\n        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"id":"vFgqxTGSI_Ip","execution":{"iopub.status.busy":"2021-12-06T08:07:54.770017Z","iopub.execute_input":"2021-12-06T08:07:54.770229Z","iopub.status.idle":"2021-12-06T08:07:54.781727Z","shell.execute_reply.started":"2021-12-06T08:07:54.770205Z","shell.execute_reply":"2021-12-06T08:07:54.780969Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The last thing to define for our `Trainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits:","metadata":{"id":"7sZOdRlRIrJd"}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(eval_predictions):\n    predictions, label_ids = eval_predictions\n    preds = np.argmax(predictions, axis=1)\n    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}","metadata":{"id":"UmvbnJ9JIrJd","execution":{"iopub.status.busy":"2021-12-06T08:07:54.785136Z","iopub.execute_input":"2021-12-06T08:07:54.785565Z","iopub.status.idle":"2021-12-06T08:07:54.795514Z","shell.execute_reply.started":"2021-12-06T08:07:54.785531Z","shell.execute_reply":"2021-12-06T08:07:54.794671Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Then we just need to pass all of this along with our datasets to the `Trainer`:","metadata":{"id":"rXuFTAzDIrJe"}},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    eval_dataset=encoded_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer),\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"imY1oC3SIrJf","execution":{"iopub.status.busy":"2021-12-06T08:07:54.798217Z","iopub.execute_input":"2021-12-06T08:07:54.799017Z","iopub.status.idle":"2021-12-06T08:07:59.877513Z","shell.execute_reply.started":"2021-12-06T08:07:54.798978Z","shell.execute_reply":"2021-12-06T08:07:59.876142Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We can now evaluate our model by just calling the `evaluate` method:","metadata":{"id":"CdzABDVcIrJg"}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T08:07:59.879105Z","iopub.execute_input":"2021-12-06T08:07:59.879369Z","iopub.status.idle":"2021-12-06T08:08:19.972259Z","shell.execute_reply.started":"2021-12-06T08:07:59.879330Z","shell.execute_reply":"2021-12-06T08:08:19.971483Z"},"trusted":true},"execution_count":11,"outputs":[]}]}